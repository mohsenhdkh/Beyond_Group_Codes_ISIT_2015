


\section{Group Codes with $\{ 0,1\}$ generator matrices}\label{sec: group codes with 0,1 gen mtx}
In this draft, we will show that the ensemble of uniform random group codes in $\ZZ_4$ with $\{ 0,1\}$-generator matrices followed by a group code with rate approaching to 2, can achieve the following rate region over any discrete memoryless channel with input $\ZZ_4$:

\begin{align*}
R&<2-H(X|Y)\\
R&< 2-H(X|Y,[X]_{\{ 0,2\}})
\end{align*}

where $[X]_{\{ 0,2\}}$ denotes the mapping from $\ZZ_4$ to $\ZZ_4/2\ZZ_4$ with $x\rightarrow x+2\ZZ_4$. 

Moreover, we propose a more general construction of such group codes by adding an extra component. We show that using the proposed construction we can achieve a larger rate region. 

\begin{definition}
Suppose $\mathcal{X}=\{\alpha_1,\alpha_2,\cdots, \alpha_N \}$ where $N$ is a positive integer and $\alpha_i$'s are distinct. For a sequence $x^n\in \mathcal{X}^n$ define it's n-type as $t_n(x^n):=(t(x^n,\alpha_1),t(x^n,\alpha_2),\cdots, t(x^n,\alpha_N))$ where $t(x^n,\alpha_i):= \frac{1}{n} |\{ j: x_j=\alpha_i\}|$. Let $\mathcal{T}_n(\mathcal{X})$ be the set of all n-types on $\mathcal{X}$. Also, let $\mathcal{P}(X)$ denotes the set of all probability distributions on $\mathcal{X}$. For $V\in \mathcal{T}_n(\mathcal{X})$, define $T_{\mathcal{X}}^n(V):=\{ x^n\in \mathcal{X}^n|t(x^n)=V\}$.
\end{definition}

\begin{definition}
For random variable $X$ taking values in $\mathcal{X}$ and  $\epsilon>0$, define $A_\epsilon^n(X)$ the set of all typical sequences with length $n$. If $P_X$ is the probability distribution of $X$, for $\delta>0$, define: 

\begin{equation*}
\mathcal{T}_\delta^n(X):=\{ V_X\in \mathcal{T}_n(\mathcal{X}) : ~ \|V_X(x)-P_X(x)\|<\delta, \forall x\in \mathcal{X}\} 
\end{equation*}

Similarly, for a pair of random variables $X,Y$,  define $\mathcal{T}_\delta^n(X,Y)$ and $\mathcal{T}_\delta^n(X|Y)$. 
\end{definition}

\begin{definition}
For a matrix $\mathbf{G}\in \{0,1\}^{k \times n}$ and a vector $b^n\in \ZZ_4^n$ define  
\begin{equation}
\mathcal{C}:=\{ u^k\mathbf{G}+b^n|u^k\in \ZZ_4^k\}
\end{equation}

Note $\mathcal{C}$ represents the codebook of the group code over $\ZZ_4$ generated by $\mathbf{G}$ with dither $b^n$. 
\end{definition}

Let $(\mathcal{X},\mathcal{Y}, W)$ denote a discrete memoryless channel with input alphabet $\mathcal{X}$, output alphabet $\mathcal{Y}$ and conditional probability distribution $W \in \mathcal{P}(X|\mathcal{Y})$. 

Given message $u^k$ the encoder sends $u^k\mathbf{G}+b^n$. The decoding can be performed by typicality decoding. Upon receiving the channel's output sequence $y^n$ the decoder declares error if there is no code word in $\mathcal{C}$ which is jointly typical with $y^n$ or if there are more than one of such codewords. 

Suppose that the message sequences $u^k$ are chosen  equally likely. Thus for a fixed $\mathbf{G}$ and $b^n$ the average error probability of $\mathcal{C}$ for channel $(\mathcal{X}=\ZZ_4, \mathcal{Y},W)$ is

\begin{align*}
P_{err}(\mathbf{G},b^n):=\sum_{u^k\in \ZZ_4^k}\frac{1}{4^k} \sum_{x^n\in A_\epsilon^n(X)} \mathbbm{1}\{ u^k \mathbf{G}+b^n=x^n\} \sum_{y^n\in A_\epsilon^n(Y|x^n)}  W^n(y^n|x^n) \mathbbm{1}\{ \exists ~ \tilde{x}^n\in \mathcal{C} \cap A_\epsilon^n(X|y^n), \tilde{x}^n\neq x^n \}
\end{align*}

Note
\begin{align*}
\mathbbm{1}\{ \exists ~ \tilde{x}^n\in \mathcal{C} \cap A_\epsilon^n(X|y^n), \tilde{x}^n\neq x^n \}=\sum_{\substack{\tilde{u}^k \in \ZZ_4^k \\\tilde{u}^k\neq u^k}}\sum_{\tilde{x}^n\in A_\epsilon^n(X|y^n)}\mathbbm{1}\{ \tilde{u}^k\mathbf{G}+b^n=\tilde{x}^n\}
\end{align*}

Now consider the ensemble of all group codes with generator matrix $\mathbf{G}$ of size $k \times n$ and dither $b^n$. Suppose elements of $\mathbf{G}$  are independently and uniformly chosen from the set $\{ 0,1\}$. Also let $b_i$'s be chosen randomly, independent of $\mathbf{G}$ with uniform distribution over $\ZZ_4$. Then the average error probability over all such codes is
.
\begin{align}\label{equ: p_err}
P_{err}&=\EE\{ P_{err}(\mathbf{G},b^n) \}\\
&=\sum_{u^k\in \ZZ_4^k} \sum_{\substack{\tilde{u}^k \in \ZZ_4^k \\\tilde{u}^k\neq u^k}} \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\tilde{x}^n\in A_\epsilon^n(X|y^n)} \frac{1}{4^k} W^n(y^n|x^n) \PP\{ \tilde{u}^k\mathbf{G}+b^n=\tilde{x}^n, u^k \mathbf{G}+b^n=x^n\} 
\end{align}


Since $b^n$ is uniform over $\ZZ_4^n$ then 

\begin{align} \label{equ: p_u}
\PP\{ \tilde{u}^k\mathbf{G}+b^n=\tilde{x}^n, u^k \mathbf{G}+b^n=x^n\}=\frac{1}{4^n}\PP\{ (\tilde{u}^k-u^k)\mathbf{G}=(\tilde{x}^n-x^n)\}
\end{align}

Assume $t_k(\tilde{u}^k-u^k)=q$. Depending on $q$ there are three cases for  $\PP\{ (\tilde{u}^k-u^k)\mathbf{G}=(\tilde{x}^n-x^n)\}$:

\begin{enumerate}
\item[Case 1:] $q(1)+q(3)=0$ i.e., $(\tilde{u}^k-u^k)\in 2\ZZ_4^k$.
\item[Case 2:] $q(1)+q(3)>0, q(2)>0$.
\item[Case 3:] $q(1)+q(3)>0, q(2)=0$.
\end{enumerate}

 The following lemma considers Case 1 and 2. 
\begin{lem}\label{lem: p_u}
 If $u^k \in \ZZ_4^k, x^n\in \ZZ_4^n$ are two none zero sequences then the following hold for  $P:=\PP\{ u^k\mathbf{G}=x^n\}$
\begin{enumerate}
\item If $u^k\in 2 \ZZ_4^k$ then $P=\frac{1}{2^n}$ for $x^n \in 2 \ZZ_4^n$ otherwise $P=0$.
\item If  $u^k \notin 2 \ZZ_4^k$,  $t_k(u^k, 2)>0$ then  $P=\frac{1}{4^n}$.
\end{enumerate}
\end{lem}

\begin{proof}
For the first statement note that if $u^k\in 2 \ZZ_4^k$ then $ u^k\mathbf{G}\in 2 \ZZ_4^k$. Thus $P=0$ when $x^n \notin 2 \ZZ_4^n$. As $u^k\in 2\ZZ_4^k-0^k$,without loss of generality assume $u_1=2$. Therefore, $P$ can be written as $\PP\{ 2\mathbf{G}_1=x^n-\sum_{i>1} u_i \mathbf{G}_i\}$ where $\mathbf{G}_i$ is the $i^{th}$ row of $\mathbf{G}$. Hence in this case, $P=\frac{1}{2^n}$


The proof for 2. is similar. Since $u^k \notin 2 \ZZ_4^k$ and $t(u^k, 2)>0$, there exist two indices $i,i'$ with $u_{i}=2$ and $u_{i'}=1$ or $3$. Without loss of generality let $i=1, i'=2$. It can be shown that $2\mathbf{G}_1+\mathbf{G}_2$ and $2\mathbf{G}_1+3\mathbf{G}_2$ are uniform over $\ZZ_4^n$. Thus if $u_2=1$,  $P=\PP\{ 2\mathbf{G}_1+\mathbf{G}_2=x^n-\sum_{i>2} u_i \mathbf{G}_i\}=\frac{1}{4^n}$. The same result holds if $u_2=3$.
\end{proof}

For the third case we need another lemma.

\begin{lem}\label{lem: p_u case 3}
Suppose $Z^n=u^k\mathbf{G}$; where elements of $\mathbf{G}$ are chosen randomly, uniformly and independently of each other from $\{ 0,1 \}$.  Then $Z_i$ are i.i.d and if $t_k(u^k)=q, t_n(x^n)=r$ then
\begin{equation}
\PP\{ Z^n=x^n\}=2^{-nD(r\| Q_{k,q})-nH(r)}
\end{equation}
where $Q_{k,q}$ is the distribution of $Z_i$. Moreover $Q_{k,q}$ depends on $u^k$ only through its type.
\end{lem}

\begin{proof}
Since elements of $\mathbf{G}$ are independent, $Z_i$'s are also independent. By definition $Z_i=\sum_{j=1}^{k} u_j\mathbf{G}_{ji}$. For $z\in \ZZ_4$, let $I_z:=\{l: u_l=z\}$. Then, $Z_i=\sum_{z=\ZZ_4} \sum_{l\in I_z} z\mathbf{G}_{li}$.  Hence, as $\mathbf{G}_{li}$'s have identical distributions, $Z_i$'s also have identical distributions. Thus, $Z_i$'s are i.i.d. For simplicity, let $Z$ denote any of such $Z_i$'s. The distribution of $Z$ only depends on the size of $I_z$'s. Since $|I_z|=kq(z)$, we can denote the distribution of $Z$ by $Q_{k,q}$, i.e., $\PP\{Z_i=x\}=\PP\{Z=x\}:=Q_{k,q}(x)$ for $x\in \ZZ_4$. Therefore, given the type of $x^n$, we have
\begin{align*}
\PP\{ Z^n=x^n\}&=\prod_{z\in \ZZ_4}\PP\{ Z=z\}^{nr(z)}\\
&=\prod_{z\in \ZZ_4}Q_{k,r}(z)^{nr(z)}\\
&=2^{n\sum_{z\in \ZZ_4}{r(z)\log_2(Q_{k,r}(z))} }
\end{align*}

Note that

\begin{align*}
\sum_{z\in \ZZ_4}{r(z)\log_2(Q_{k,r}(z))}&=\sum_{z\in \ZZ_4}{r(z)\log_2(\frac{Q_{k,r}(z)}{r(z)})}+\sum_{z\in \ZZ_4}{r(z)\log_2(r(z))}\\
&=-D(r\| Q_{k,q})-H(r)
\end{align*}
This completes the proof.
\end{proof}


Let $S_q:=\{q\in \mathcal{T}_k(\ZZ_4) | q(1)+q(3)>0, q(2)>0 \}$ and $\tilde{S}_q:=\{q\in \mathcal{T}_k(\ZZ_4) | q(1)+q(3)>0, q(2)=0 \}$.  (\ref{equ: p_err}) is divided into three terms: 
\begin{align} \label{equ: p_err_1}\nonumber
P_{err}&=
\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k \\ (\tilde{u}^k-u^k)\in 2\ZZ_4^k}} \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)} } \frac{1}{4^k}\frac{1}{4^n} W^n(y^n|x^n)\PP\{ (\tilde{u}^k-u^k)\mathbf{G}=(\tilde{x}^n-x^n)\}\\\nonumber
%
&+\sum_{q\in S_q}\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k \\ t(\tilde{u}^k-u^k)=q}} \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n \in A_\epsilon^n(X|y^n)} } \frac{1}{4^k}\frac{1}{4^n}  W^n(y^n|x^n) \PP\{ (\tilde{u}^k-u^k)\mathbf{G}=(\tilde{x}^n-x^n)\}\\
%
&+\sum_{q\in \tilde{S}_q}\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k \\ t(\tilde{u}^k-u^k)=q}} \sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)\\t(\tilde{x}^n-x^n)=r} } \frac{1}{4^k}\frac{1}{4^n} W^n(y^n|x^n)\PP\{ (\tilde{u}^k-u^k)\mathbf{G}=(\tilde{x}^n-x^n)\}
\end{align}


Applying  lemma \ref{lem: p_u} for the first two terms of (\ref{equ: p_err_1}) and lemma \ref{lem: p_u case 3} for the third term yield: 
\begin{align}\label{p_err_part 1}
P_{err}&=
\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k \\ (\tilde{u}^k-u^k)\in 2\ZZ_4^k}} \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)\\(\tilde{x}^n-x^n)\in 2\ZZ_4^n} } \frac{1}{4^k}\frac{1}{4^n} \frac{1}{2^n} W^n(y^n|x^n)\\\label{p_err_part 2}
%
&+\sum_{q\in S_q}\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k \\ t(\tilde{u}^k-u^k)=q}} \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n \in A_\epsilon^n(X|y^n)} } \frac{1}{4^k}\frac{1}{4^n} \frac{1}{4^n} W^n(y^n|x^n)\\\label{p_err_part 3}
%
&+\sum_{q\in \tilde{S}_q}\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k \\ t_k(\tilde{u}^k-u^k)=q}} \sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)\\t_n(\tilde{x}^n-x^n)=r} } \frac{1}{4^k}\frac{1}{4^n} W^n(y^n|x^n)2^{-nD(r\| Q_{k,q})-nH(r)}
\end{align}


Define the rate of the code as $R=\frac{2k}{n}$.
\begin{lem} \label{lem: part 1&2}
(\ref{p_err_part 1}) and (\ref{p_err_part 2}) converge to $0$ if 
\begin{align*}
R&<2-H(X|Y)\\
R&<2-H(X|Y,[X]_{\{ 0,2\}})
\end{align*}
\end{lem}


\begin{proof}
The proof is straightforward. 
\end{proof}

Denote the third term in $P_{err}$, i.e., (\ref{p_err_part 3}),  by $P_{err,3}$. Having Lemma \ref{lem: part 1&2}, it remains to find the necessary conditions on $R$ to let $P_{err,3}$ converge to zero. By Lemma \ref{lem: p_u case 3}, since the inner terms of the sums in $P_{err,3}$ depend on $u^k, \tilde{u}^k$ only through type $q$, we can replace the summation over $u^k, \tilde{u}^k$ by $|\{ (u^k, \tilde{u}^k):t(\tilde{u}^k-u^k)=q \}|=4^k 2^{kH(q)}$. Therefore, we have:

\begin{align*}
P_{err,3}=\sum_{q\in \tilde{S}_q}\sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)\\t_n(\tilde{x}^n-x^n)=r} } \frac{1}{4^n} W^n(y^n|x^n)2^{-nD(r\| Q_{k,q})-nH(r)+kH(q)}
\end{align*} 

Summations over $x^n, y^n$ and $\tilde{x}^n$ can be replaced by  two summations; one on a set of joint types over $(\mathcal{X}\times \mathcal{Y})$ and $(\tilde{\mathcal{X}} \times \mathcal{Y})$ and the other over triples $(x^n, y^n, \tilde{x}^n)$ in the following manner:


\begin{lem}\label{lem: p_err3, F_n}
For joint types $V_{XY}\in \mathcal{T}_n(\mathcal{X}\times \mathcal{Y}), W_{\tilde{X}Y} \in \mathcal{T}_n(\tilde{\mathcal{X}}\times \mathcal{Y}) $ and $r\in \mathcal{T}_n(\mathcal{X})$, define:
\begin{equation*}
F_n(V_{XY},W_{\tilde{X}Y},r):=\{(x^n,y^n,\tilde{x}^n): t_n(x^n,y^n)=V_{XY}, t_n(\tilde{x}^n,y^n)=W_{XY}, t_n(\tilde{x}^n-x^n)=r \}
\end{equation*}

Then $P_{err,3}$ can be written as:

\begin{align} \label{equ: p_err3, F_n}
P_{err,3}=\sum_{q\in \tilde{S}_q}\sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }} \sum_{(x^n,y^n,\tilde{x}^n)\in F_n(V_{XY},W_{\tilde{X}Y},r)}  \frac{1}{4^n} W^n(y^n|x^n)2^{-nD(r\| Q_{k,q})-nH(r)+kH(q)}
\end{align}

\end{lem}
\begin{proof}
$P_{err,3}$ can be written as

\begin{align*}
P_{err,3}=\sum_{q\in \tilde{S}_q}\sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{(x^n,y^n,\tilde{x}^n)\in B_\epsilon^n(X,Y,\tilde{X},r)}  \frac{1}{4^n} W^n(y^n|x^n)2^{-nD(r\| Q_{k,q})-nH(r)+kH(q)}
\end{align*}

where
\begin{align*}
B_\epsilon^n(X,Y,\tilde{X},r):=\{ (x^n,y^n,\tilde{x}^n)| (x^n,y^n)\in A_\epsilon^n(X,Y),(\tilde{x}^n,y^n)\in A_\epsilon^n(\tilde{X},Y),  t_n(\tilde{x}^n-x^n)=r \}
\end{align*}



 Summation over triple $(x^n,y^n,\tilde{x}^n)$ can be divided into two summations as follows:

\begin{align*}
P_{err,3}=\sum_{q\in \tilde{S}_q}\sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}|Y}\in  \mathcal{T}_\epsilon^n(\tilde{X}|Y) }} \sum_{(x^n,y^n,\tilde{x}^n)\in G_n(V_{XY},W_{\tilde{X}|Y},r)}  \frac{1}{4^n} W^n(y^n|x^n)2^{-nD(r\| Q_{k,q})-nH(r)+kH(q)}
\end{align*}

where

\begin{equation*}
G_n(V_{XY},W_{\tilde{X}|Y},r):=\{(x^n,y^n,\tilde{x}^n): t_n(x^n,y^n)=V_{XY}, t_n(\tilde{x}^n|y^n)=W_{\tilde{X}|Y}, t_n(\tilde{x}^n-x^n)=r \}
\end{equation*}

Suppose that the conditional type induced by $W_{\tilde{X}Y}$ is $W_{\tilde{X}|Y}$. Let $V_Y, W_Y$ be the marginal types of $V_{XY}$ and $W_{\tilde{X}Y}$, respectively. If $V_Y \neq W_Y$ then, $F_n(V_{XY},W_{\tilde{X}Y},r)=\emptyset$; otherwise, $F_n(V_{XY},W_{\tilde{X}Y},r)=G_n(V_{XY},W_{\tilde{X}|Y},r)$. Therefore, $P_{err,3}$ can be written as in (\ref{equ: p_err3, F_n}) and the proof is complete.
\end{proof}


For joint types $V_{XY},W_{\tilde{X}Y}$ and type $r$, define:
\begin{equation*}
S_n(V_{XY},W_{\tilde{X}Y},r):=\{ U_{XY\tilde{X}} \in \mathcal{T}_n(\mathcal{X},\mathcal{Y},\tilde{\mathcal{X}})| U_{XY}=V_{XY}, U_{\tilde{X}Y}=W_{\tilde{X}Y}, U_{\tilde{X}-X}=r\}
\end{equation*}

\begin{lem}\label{lem: F_n}
 The following bound on the size of $F_n(V_{XY},W_{\tilde{X}Y},r)$ holds:

\begin{align*}
| F_n(V_{XY},W_{\tilde{X}Y},r) | \leq 2^{nH_{s,n}(V_{XY},W_{\tilde{X}Y},r)+O(\log n)}
\end{align*}

where

\begin{equation*}
H_{s,n}(V_{XY},W_{\tilde{X}Y},r):= \max_{a\in S_n(V_{XY},W_{\tilde{X}Y},r)} H(a)
\end{equation*}
\end{lem}

\begin{proof}
\begin{equation*}
| F_n(V_{XY},W_{\tilde{X}Y},r) | =\sum_{a\in S_n(V_{XY},W_{\tilde{X}Y},r)} |T_{XY\tilde{X}}^n(a)|
\end{equation*}

Note 
\begin{equation*}
|S_n(V_{XY},W_{\tilde{X}Y},r)| \leq \begin{pmatrix}
n+|\mathcal{X}||\tilde{\mathcal{X}}||\mathcal{Y}|-1 \\ |\mathcal{X}||\tilde{\mathcal{X}}||\mathcal{Y}|-1 
\end{pmatrix}
\end{equation*}

Moreover, as $|T_{XY\tilde{X}}^n(a)| \approx 2^{nH(a)}$ and $|S_n(V_{XY},W_{\tilde{X}Y},r)| \leq (n+|\mathcal{X}||\tilde{\mathcal{X}}||\mathcal{Y}|-1)^{|\mathcal{X}||\tilde{\mathcal{X}}||\mathcal{Y}|-1} $, we have:

\begin{equation*}
| F_n(V_{XY},W_{\tilde{X}Y},r) | \leq (n+|\mathcal{X}||\tilde{\mathcal{X}}||\mathcal{Y}|-1)^{|\mathcal{X}||\tilde{\mathcal{X}}||\mathcal{Y}|-1}  ~2^{nH_s(V_{XY},W_{\tilde{X}Y},r)}
\end{equation*}


If $|\mathcal{Y}|< \infty$, $|S_n(V_{XY},W_{\tilde{X}Y},r)|$ grows polynomially as $n$ increases and can be ignored to get
\begin{align*}
| F_n(V_{XY},W_{\tilde{X}Y},r) | \leq 2^{nH_s(V_{XY},W_{\tilde{X}Y},r)}
\end{align*}

\end{proof}

For more convenience in studying $H_{s,n}(V_{XY},W_{\tilde{X}Y},r)$, we drop the condition that elements of $S_n(V_{XY},W_{\tilde{X}Y},r)$ should be types. This leads us to define 
\begin{equation}\label{equ: H_S_S}
H_{s}(V_{XY},W_{\tilde{X}Y},r):= \max_{a\in S(V_{XY},W_{\tilde{X}Y},r)} H(a)
\end{equation}

where

\begin{equation}\label{equ: H_S}
S(V_{XY},W_{\tilde{X}Y},r):=\{ U_{XY\tilde{X}} \in \mathcal{P}(\mathcal{X},\mathcal{Y},\tilde{\mathcal{X}})| U_{XY}=V_{XY}, U_{\tilde{X}Y}=W_{\tilde{X}Y}, U_{\tilde{X}-X}=r\}
\end{equation}
Clearly the bound on Lemma \ref{lem: F_n} holds by replacing $H_{s,n}(\cdot)$ with $H_s(\cdot)$.

We need the following claim to show that the terms in the most inner summation at (\ref{equ: p_err3, F_n}) depend on triple $(x^n,y^n,\tilde{x}^n)$ only through types $V_{XY},W_{\tilde{X}Y}$ and $r$.

\begin{claim}\label{claim: Wn}
If $t_n(x^n,y^n)=V_{XY}$ then 
\begin{equation*}
\frac{1}{4^n}W^n(x^n|y^n) = 2^{-nD(V_{XY}\| P_{XY})} 2^{-nH(V_{XY})} 
\end{equation*}
where $P_{XY}$ is the joint probability distribution of $XY$ such that $P_{XY}(x,y)=\frac{1}{4}W(y|x)$.

\end{claim}

\begin{proof}
By the definition of $P_{XY}$,
\begin{align*}
\frac{1}{4^n}W^n(x^n|y^n)&=\prod_{i=1}^nP_{XY}(x_i,y_i)\\
&=\prod_{(x,y)\in \ZZ_4^2}P_{XY}(x,y)^{nV_{XY}(x,y)}\\
&= 2^{-nD(V_{XY}\| P_{XY})} 2^{-nH(V_{XY})}
\end{align*}
where the last equality can be shown by a similar argument as in the proof of Lemma \ref{lem: p_u case 3}.
\end{proof}


Now having Lemma \ref{lem: p_err3, F_n} and Claim \ref{claim: Wn},  the summation over $(x^n,y^n,\tilde{x}^n)$ in (\ref{equ: p_err3, F_n}) can be replaced by  $| F_n(V_{XY},W_{\tilde{X}Y},r) | $. Hence, by Lemma \ref{lem: F_n} we have:

\begin{align} \label{equ: p_err3, V_XY,W_XY}
P_{err,3}\leq \sum_{q\in \tilde{S}_q}\sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }} 2^{-nD(r\| Q_{k,q})-nH(r)+kH(q)} ~2^{nH_s(V_{XY},W_{\tilde{X}Y},r)}~ 2^{-nD(V_{XY}\| P_{XY})-nH(V_{XY})+O(\log n)}
\end{align}


\begin{definition}
Consider sets $\mathcal{A,B,C}$. For $\epsilon>0$, define the $\delta$-\textit{neighborhood} of triple types $(V_0,W_0,r_0)\in \mathcal{T}_n(\mathcal{A,B,C})$ as:
\end{definition}


\begin{equation*}
\mathcal{N}_\delta(V_{XY},W_{\tilde{X}Y},r):=\Big\{ (V,W,r)\in \mathcal{T}_n(\mathcal{A,B,C}): |V-V_0|\leq \delta, |W-W_0|\leq \delta, |r-r_0|\leq \delta \Big \}
\end{equation*}
\begin{claim}\label{claim: H_s}
For any triple types $(V_{XY},W_{\tilde{X}Y},r)$ there exists $\delta >0$ such that for any $(V,W,\tilde{r}) \in \mathcal{N}_\delta(V_{XY},W_{\tilde{X}Y},r)$,  $H_s(V,W,\tilde{r})$ is second order differentiable .  
\end{claim}


\begin{proof}
The proof is similar to Zhang $\&$ Yang approach and is given at Appendix B.
\end{proof}


Therefore, by the above Claim and since the entropy function and the Kullback–Leibler divergence are continuous, (\ref{equ: p_err3, V_XY,W_XY}) can be changed to:

\begin{align} \label{equ: p_err3, P_XY}
P_{err,3}\leq \sum_{q\in \tilde{S}_q}\sum_{r\in\mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }} 2^{-nD(r\| Q_{k,q})-nH(r)+kH(q)} ~2^{nH_s(P_{XY},P_{\tilde{X}Y},r)}~ 2^{-nH(P_{XY})+O(\log n)}
\end{align}

Note that by definition, $P_{\tilde{X}Y}=P_{XY}$. The terms in (\ref{equ: p_err3, P_XY}) do not depend on types $V_{XY},W_{\tilde{X}Y} $ and since the size of $\mathcal{T}_\epsilon^n(X,Y)$ and $\mathcal{T}_\epsilon^n(\tilde{X},Y) $ grow polynomially with $n$, we can remove the corresponding summation on $V_{XY},W_{\tilde{X}Y} $. Finally, we have:

\begin{align*}
P_{err,3}\leq 2^{nE(n,R)}
\end{align*}

where

\begin{align*}
E(n,R)= \max_{q\in \tilde{S}_q, r\in\mathcal{T}_n(\mathcal{X})} -D(r\| Q_{k,q})-H(r)+\frac{k}{n} H(q)+H_s(P_{XY},P_{XY},r)-H(P_{XY})+O(\frac{\log n}{n})
\end{align*}


To calculate $E(n,R)$ we need to study $ Q_{k,q}$.
\begin{lem}\label{lem: Q_nr convergence}
If $kq(1)\rightarrow \infty$ or $kq(3)\rightarrow \infty$ as $k \rightarrow \infty$ then, $Q_{k,q}$ converge to a uniform distribution on $\ZZ_4$ as $k \rightarrow \infty$.
\end{lem}

\begin{proof}
The proof is given in the appendix.
\end{proof}

By Lemma \ref{lem: Q_nr convergence} there are two cases for $E(n,R)$ :

\begin{enumerate}
\item $q(1)$ or $q(3)$ satisfies the condition in Lemma \ref{lem: Q_nr convergence}.
\item $q(1),q(3)=O(\frac{1}{k})$ . 
\end{enumerate}


\begin{lem}\label{lem: p_err3, Case 1}
In case 1, for large enough $n$ 
\begin{equation*}
E(n,R) \leq  \frac{R}{2}\log_2(3) -2+H(X|Y) +O(\frac{\log n}{n})
\end{equation*}

\end{lem}


\begin{proof}
%\textbf{Case 1:}%
Note since $R$ is bounded by Lemma \ref{lem: part 1&2}, if $n$ is large then so is $k$. Using Lemma \ref{lem: Q_nr convergence}, for large enough $k$, $Q_{k,q}$ is uniform and it gives:

\begin{equation*}
D(r\| Q_{k,q}) \approx 2-H(r)
\end{equation*}

Consequently:

\begin{align*}
E(n,R)&= \max_{q\in \tilde{S}_q, r\in\mathcal{T}_n(\mathcal{X})} -2+\frac{k}{n} H(q)+H_s(P_{XY},P_{XY},r)-H(P_{XY})\\
&\leq\max_{ r\in\mathcal{T}_n(\mathcal{X})} -2+\frac{k}{n} \log_2(3)+H_s(P_{XY},P_{XY},r)-H(P_{XY})
\end{align*}

Using the definition of $H_s(\cdot)$ in (\ref{equ: H_S}), we have:

\begin{equation*}
H_{s}(P_{XY},P_{XY},r):= \max_{q_{XY\tilde{X}}\in S(P_{XY},P_{XY},r)} H(XY\tilde{X})
\end{equation*}

Therefore, $E(n,R)$ is
\begin{align*}
E(n,R)&\leq -2+\frac{k}{n} \log_2(3)+\max_{ r\in\mathcal{T}_n(\mathcal{X}), q_{XY\tilde{X}}\in S(P_{XY},P_{XY},r)} H(XY\tilde{X})-H(XY)
\end{align*}

Note that

\begin{align*}
H(XY\tilde{X})-H(XY)=H(\tilde{X}|XY)\leq H(\tilde{X}|Y)
\end{align*}
But as $q_{XY\tilde{X}}\in S(P_{XY},P_{XY},r)$, $q_{\tilde{X}Y}=P_{XY}$. Hence, $H(\tilde{X}|Y)=H(X|Y)$. Consequently

\begin{align*}
E(n,R)&\leq -2+\frac{k}{n} \log_2(3)+\max_{ q_{XY\tilde{X}}\in S(P_{XY},P_{XY},r)} H(X|Y)
&=-2+\frac{k}{n} \log_2(3)+H(X|Y)
\end{align*}

The last equality is true as $q_{XY}$ is taken to be equal to $P_{XY}$. 


%
%\textbf{Case 2:} $q(1),q(3)=O(\frac{1}{n})$.
%
%As $q(2)=0$ and $\frac{k}{n}$ is bounded by Lemma \ref{lem: part 1&2}, for large enough $n$ the term $\frac{k}{n}H(q)$ vanishes. Hence we have:
%\begin{align*}
%E(n,R)= \max_{q\in \tilde{S}_q, r\in\mathcal{T}_n(\mathcal{X})} -D(r\| Q_{k,q})-H(r)+H_s(P_{XY},P_{XY},r)-H(P_{XY})
%\end{align*}
%By a similar argument as in Case 1
%\begin{align*}
%E(n,R)= \max_{q\in \tilde{S}_q, r\in\mathcal{T}_n(\mathcal{X})} \max_{q_{XY\tilde{X}}\in S(P_{XY},P_{XY},r)} -D(r\| Q_{k,q})-H(r)+H(XY\tilde{X})-H(XY)
%\end{align*}
%
%Let random variable $W=\tilde{X}-X$. Clearly $P_W=r$, thus
% 
% \begin{align*}
% H(XY\tilde{X})-H(XY)-H(r)=H(XYW)-H(XY)-H(W)=-I(XY;W)
% \end{align*}
%Thus
% \begin{align*}
%E(n,R)\leq \max_{q\in \tilde{S}_q, P_W\in\mathcal{T}_n(\mathcal{X})} \max_{q_{XY\tilde{X}}\in S(P_{XY},P_{XY},P_W)} -D(P_W\| Q_{k,q})-I(XY;W)
%\end{align*}
%
%It can be shown that in this case $E(n,R)<0$. The argument is as follows:
%
%By contradiction suppose $E(n,R)=0$ then,
%
%\begin{eqnarray}
%I(XY;W)=0\\
%D(P_W\| Q_{k,q})=0
%\end{eqnarray}
% 
% for some $q, P_W, q_{XY\tilde{X}}$. This means, $W$ is independent of $XY$ and $P_W=Q_{k,q}$. Since $W \perp XY$, $W \perp X$. As $X$ is assumed to be uniform, $\tilde{X}=W+X$ is also uniform
\end{proof}
 
%\begin{corollary}
%$P_{err}$ converges to $0$ as $n\rightarrow \infty$ if 
%\begin{align*}
%R&<2-H(X|Y)\\
%R&<2-H(X|Y,[X]_{\{ 0,2\}})
%\end{align*}
%\end{corollary}

For Case 2, ($q(1)+q(3)=O(1/k)$), $E(n,R)$ is $O(\frac{\log n}{n})$ and thus $P_{err,3}$ may not approach to $0$. To address this issue, first let to define a distance on group $\ZZ_4$ that is an extension of the \textit{Hamming distance}  to $\ZZ_4$:

 \begin{align*}
d_H&: \ZZ_4\rightarrow \{ 0,1\}\\
d_H(x,y)&=1 \quad if \quad x \neq y, d(x,x)=0
\end{align*}

Define the Hamming distance of the sequences $u^k,\tilde{u}^k$ as $d_H(u^k,\tilde{u}^k):= \sum_i d_H(u_i,\tilde{u}_i)$. Also, define \textit{relative Hamming distance} of $u^k,\tilde{u}^k$ as  $\delta_H(u^k,\tilde{u}^k):=1/k d_H(u^k,\tilde{u}^k)$. 

Note that we can write $P_{err,3}$, in case 2 as: 

\begin{align*}
P_{err,3, case 2}=\sum_{u^k} \frac{1}{4^k}  \sum_{q~\in~ Case 2 } \quad\sum_{\tilde{u}^k \in\mathcal{ B}_q(u^k)} \EE_{\mathbf{G},b^n}\{\PP(g(Y^n)=\tilde{u}|X^n=u^k \mathbf{G}+b^n)\}
\end{align*}

where $g:\mathcal{Y}^n\rightarrow \mathcal{U}^k$ is the decoder and 
$$\mathcal{ B}_q(u^k):=\{\tilde{u}^k : t_k(\tilde{u}^k-u^k)=q \}$$

Let $\mathcal{D}_\delta(u^k):=\{\tilde{u}^k: \tilde{u}^k \neq u^k, \delta_H(u^k,\tilde{u}^k)\}=\delta\}$, clearly $\mathcal{ B}_q(u^k) \subset \mathcal{D}_{q(1)+q(3)}(u^k)$ thus we can upper bound $P_{err,3, case 2}$ as

\begin{align*}
P_{err,3, case 2}&<\sum_{u^k} \frac{1}{4^k}  \sum_{q~\in~ Case 2 } \quad\sum_{\tilde{u}^k \in\mathcal{ D}_q(u^k)} \EE_{\mathbf{G},b^n}\{\PP(g(Y^n)=\tilde{u}|X^n=u^k \mathbf{G}+b^n)\}\\
&\leq \sum_{u^k} \frac{1}{4^k}  \sum_{q~\in~ Case 2 } |\mathcal{D}_{q(1)+q(3)}(u^k)| \times 1
\end{align*}

In order to have $P_{err,3, case 2}\rightarrow 0$, we will use a code $\mathcal{C}_{precode}$ to encode the message before using the original code $\mathcal{C}$. In this case, $u^k, \tilde{u}^k$'s will be codewords of   $\mathcal{C}_{precode}$ .  This results in a two consequence encoding schemes. We set $\mathcal{C}_{precode}$ to be a group code with matrix $\mathbf{G}_{pre}$ with elements in  $\ZZ_4$ and size $l\times k$. If $v^l$ is the message sequence then the output of the two-encoder is $x^n$ such that:

\begin{align*}
x^n&=u^k\mathbf{G}\\
u^k&=v^l\mathbf{G}_{pre}
\end{align*} 

Since $\mathcal{C}_{precode}$ is a group code, $|\mathcal{D}_{q(1)+q(3)}(u^k)|$ does not depend on $u^k$ and we will denote it by $D_{q(1)+q(3)}$. Therefore $P_{err,3, case 2}< \sum_{q~\in~ Case 2 } D_{q(1)+q(3)}$. In addition, to let  $P_{err,3, case 2}\rightarrow 0$, it suffices to set $\mathcal{C}_{precode}$ in a way that its relative minimum distance be $\delta_k = O(1/\sqrt{k})$.  Note that in this case, the bounds derived in Lemma \ref{lem: part 1&2} and  \ref{lem: p_err3, Case 1} still hold. Thus the total error probability $P_{err}$ converges to zero and we are done. The following Lemma shows the existence of such code and it derives its rate:

\begin{lem}\label{lem: code_existence}
For large enough $k$, there exists a group code in $\ZZ_4$ with length $k$, relative minimum distance  $\delta$ and rate $R< 2-H(\delta)-\delta \log_2 3 $. 
\end{lem}

\begin{proof}

Consider the random ensemble of all group codes of the form $\mathcal{C}=\{ u^k=v^l\mathbf{G}: v^l\in \ZZ_4^l\}$. Where elements of $\mathbf{G}$ are chosen uniformly and randomly in $\ZZ_4$. In this case, for any of such group codes $D_{\delta}(u^k)$ does not depends on $u^k$. Set $u^k=\mathbf{0}$ and let $N_k(\delta):=|D_{\delta}(\mathbf{0})|$, thus by definition we get:  

\begin{align*}
N_k(\delta)=\sum_{u^k\in \mathcal{C}-\mathbf{0}} 1\{ \delta_H(u^k,\mathbf{0})=\delta \}
\end{align*}

Thus taking expectation of $N_k(\delta)$ over the above random ensemble gives:

\begin{align*}
\EE\{ N_k(\delta)\}&=\sum_{u^k\in \mathcal{C}-\mathbf{0}} \PP\{ \delta_H(u^k,\mathbf{0})=\delta \}\\
&= \sum_{v^l \in \ZZ_4^l- \mathbf{0}} \PP\{ \delta_H(v^l\mathbf{G},\mathbf{0})=\delta \}
\end{align*} 
Let $d=k\delta$. Note that if $v^l \in 2\ZZ_4 $ then 


\begin{align*}
\PP\{ \delta_H(v^l\mathbf{G},\mathbf{0})&=\delta \}= \begin{pmatrix} k\\ d \end{pmatrix} (\frac{1}{2})^{d} (\frac{1}{2})^{k-d}\\
&\approx2^{-kD(\delta \|1/2)}=2^{-k(2-H(\delta))}
\end{align*}

If $v^l \notin 2\ZZ_4 $ then 

\begin{align*}
\PP\{ \delta_H(v^l\mathbf{G},0)&=\delta \}= \sum_{\substack{d_1,d_2,d_3 \geq 0\\ d_1+d_2+d_3=d}}\begin{pmatrix} k\\ d_1,d_2,d_3  \end{pmatrix} (\frac{1}{4})^{(k-d)} \prod_i (\frac{1}{4})^{d_i}
\end{align*}

 Let $[ a_1,a_2, \cdots a_m]$ denote a probability distribution $P$ such that $P(j)=a_j, \forall j\in [1:m]$. For large enough $k$ we have


\begin{align*}
\PP\{ \delta_H(v^l\mathbf{G},0)&=\delta \}\leq \max_{\substack{\delta_1,\delta_2,\delta_3\\ \delta_1+\delta_2+\delta_3=\delta}} 2^{-k(2-H([1-\delta,\delta_1,\delta_2,\delta_3]))}
\end{align*}
By grouping axioms 
\begin{align*}
H([1-\delta,\delta_1,\delta_2,\delta_3])&= H(\delta)+\delta H([\delta_1/\delta,\delta_2/\delta,\delta_3/\delta])\\
&\leq   H(\delta)+\delta \log_2 3
\end{align*}

Consequently 
\begin{align*}
\PP\{ \delta_H(v^l\mathbf{G},0)&=\delta \}\leq  2^{-k(2- H(\delta)-\delta \log_2 3)}
\end{align*}

Now as a result

\begin{align*}
\EE\{ D_{\delta}\}&\leq 2^l 2^{-k(2-H(\delta))} + (4^l-2^l) 2^{-k(2- H(\delta)-\delta \log_2 3)}\\
& \leq  2^{-k(-l/k+1-H(\delta))} + 2^{-k(-2l/k+2- H(\delta)-\delta \log_2 3)}
\end{align*}

Hence if $R=\frac{2l}{k}< 2- H(\delta)-\delta \log_2 3$ then $\EE\{ D_{\delta}\} \rightarrow 0$ exponentially. Therefore there exist a code with rate $R$ such that $N(\delta)=0$; which means the relative minimum distance of this code is atleast $\delta$ .

%
%Consider a symmetric additive channel $Y=X+N_{\delta_k}$ where $X \in \ZZ_4$ is the input and $N_{\delta_k}$ is noise with $\PP\{N_{\delta_k}=z\}=\delta_k$ for $0 \neq z\in \ZZ_4$. Moreover let the addition be module $\ZZ_4$'s addition. It is known that the average error exponent of the ensemble of uniform random group codes over such channel is $R-I_4$ where $I_4=2-H(N_{\delta_k})=2-H(3\delta_k)-3\delta_k \log_2 3$. Since $\delta_k = O(1/\sqrt{k})$ we ignore the third term in $I_4$. Thus, setting $R\leq I_4-\Delta$, there exist a group code $\mathcal{C}$ with error exponent less than $-\Delta$.   This shows that the number of all unordered pairs of code words of $\mathcal{C}$ with relative distance less than  $\delta_k$ goes to zero exponentially as $k\rightarrow \infty$; which means $D_{\delta_k}\rightarrow 0$ exponentially as $k\rightarrow \infty$. 
\end{proof}


For $\delta=\delta_k$ set $\mathcal{C}_{precode}$ to be as described in the above  Lemma, then the rate of this code will be $\frac{l}{k}\leq 2- H(\delta_k)-\delta_k \log_2 3$ which approaches to $2$ as $k \rightarrow \infty$. 

As a result of the above argument,  the following theorem is proved.



\begin{thm}
Suppose, $\mathcal{C}_{T}=:\{v^l\mathbf{G}_{pre} \mathbf{G}+b^n | v^l\in \ZZ_4^l\} $ where $\mathbf{G}_{pre} \in \{0,1 \}^{l\times k}, \mathbf{G} \in \{ 0,1 \}^{k \times n}$. Also assume that for any  $ \delta_k = O(1/\sqrt{k})$, $\frac{l}{k} < H(\delta_k)-\delta_k \log_2 3$. Then there exists a fixed $\mathbf{G}_{pre}$ such that the average probability of error for any discrete memoryless channel $(\mathcal{X}=\ZZ_4,\mathcal{Y},W)$ over ensemble of uniform random codes of the form $\mathcal{C}_{T}$  goes to zero exponentially as $n\rightarrow \infty$ if

\begin{align*}
R&<2-H(X|Y)\\
R&< 2-H(X|Y,[X]_{\{ 0,1\}})\\
\end{align*}

where $R=\frac{2k}{n}$ and

\end{thm}


\input{Extra_component.tex}


\newpage
\appendix

\section*{Appendix}
\section{Proof of Lemma \ref{lem: Q_nr convergence}}
This Section provides a proof for Lemma \ref{lem: Q_nr convergence}. First,we need to define the followings:


\begin{definition}
For a positive integer $N$, let $x^N$ be any sequence of length $N$. $x^N[n]$ denotes the $n^{th}$ element of $x^N$starting from $0$, where $n=0,1,\cdots N-1$.

For $x^N,y^N$ define \textit{$\ZZ_N$ convolution} as:

\begin{align*}
(x^N \circledast_{\ZZ_N} y^N)[n] :=\sum_{k=0}^{N-1} x^N[k]y^N[(n-k)_N]
\end{align*}

where $(\cdot)_N$ takes module $\ZZ_N$ operation.
\end{definition}

\begin{definition}
Take a sequence $x^N$. Define its periodic version as 

\begin{equation*}
\underline{x}_N:=\sum_{k=-\infty}^\infty x^N[n-kN]
\end{equation*}

Note $\underline{x}_N$ is an infinite length sequence.
\end{definition}

\begin{definition}
For periodic sequences $\underline{x}_N,\underline{y}_N$, define \textit{circular convolution} as:

\begin{align*}
(\underline{x}_N \circledast_{N} \underline{y}_N)[n] :=\sum_{k=0}^{N-1} \underline{x}_N[k]\underline{x}_N[n-k]
\end{align*}

\end{definition}


It is clear that for $n\in \ZZ_N$

\begin{equation*}
(\underline{x}_N \circledast_{N} \underline{y}_N)[n]=(x^N \circledast_{\ZZ_N} y^N)[n] 
\end{equation*}

\begin{lem}\label{lem: Fourier Series}
Consider periodic sequences $\underline{x}_N,\underline{y}_N$ with Fourier Series' coefficients $a_k,b_k$. Let $\underline{z}_N=\underline{x}_N \circledast_{N} \underline{y}_N$, then if $c_k$ is the Fourier series' coefficient of $\underline{z}_N$ then
\begin{equation*}
c_k=Na_kb_k
\end{equation*}

\end{lem}

\begin{proof}
By definition 
\begin{align*}
c_k&=\frac{1}{N}\sum_{n=0}^{N-1} \underline{z}_N[n] e^{-j\frac{2k\pi}{N}n}\\
&=\frac{1}{N}\sum_{n=0}^{N-1} \Big(\sum_{l=0}^{N-1} \underline{x}_N[l]\underline{y}_N[n-l] \Big)e^{-j\frac{2k\pi}{N}n}\\
&=\frac{1}{N}\sum_{l=0}^{N-1} \underline{x}_N[l]\Big(\sum_{n=0}^{N-1} \underline{y}_N[n-l] e^{-j\frac{2k\pi}{N}n}\Big)\\
&=\Big(\frac{1}{N}\sum_{l=0}^{N-1} \underline{x}_N[l]e^{-j\frac{2k\pi}{N}l} \Big) Nb_k\\
&=Na_kb_k
\end{align*}
\end{proof}

\begin{lem}\label{lem: sum_of_two_rvs}
Suppose $X,Y$ are two random variables taking values in $\ZZ_N$. Let $Z=X\bigoplus_NY$, where $\bigoplus_N$ is module $\ZZ_N$ addition. If $P_X,P_Y$ and $P_Z$ are probability distributions of $X,Y$ and $Z$, respectively then, $P_Z=P_X\circledast_{\ZZ_N}P_Y$
\end{lem}

\begin{proof}
Let $P_Z[n]:=P_Z(Z=n)$ for $n\in \ZZ_N$. We have

\begin{equation*}
P_Z[n]=\sum_{k=0}^{N-1} x^N[k]y^N[(n-k)_N]=P_X\circledast_{\ZZ_N}P_Y
\end{equation*}
\end{proof}

\begin{lem}
Suppose $X_i\in \ZZ_4$, $i=1,2,\cdots m$ are i.i.d random variables with $P_{X_i}(0)=P_{X_i}(1)=\frac{1}{2}$. If $Y_m:=\sum_{i=1}^m X_i$ then, as $m\rightarrow \infty$, $Y_m$ converges in distribution to a uniform random variable over $\ZZ_4$.
\end{lem}

\begin{proof}
By Lemma \ref{lem: sum_of_two_rvs}, $P_{Y_m}= \stackrel{m}{\underset{i=1}{\Scale[2]{\circledast}_{\ZZ_4}}} P_X$. Consider the  periodic versions of $P_{Y_m}, P_X$. we have, $\underline{P_{Y_{m}}}_4= \stackrel{m}{\underset{i=1}{\Scale[2]{\circledast}_{4}}} \underline{P_{X}}_{4}$. Now if $a_k, b_k$ are Fourier series' coefficients of $\underline{P_{X}}_{4},\underline{P_{Y_{m}}}_4$, respectively then by Lemma \ref{lem: Fourier Series}
\begin{equation*}
b_k=4^{m-1}a_k^m
\end{equation*} 

Note that 
\begin{align*}
a_k&=\frac{1}{4}\sum_{n=0}^3 \underline{P_{X}}_{4}[n] e^{-j\frac{2k\pi}{N}n}\\
&=\frac{1}{4}[\frac{1}{2}+\frac{1}{2}e^{-j\frac{2k\pi}{N}}]
\end{align*}

Consequently, $b_k=\frac{1}{4}[\frac{1}{2}+\frac{1}{2}e^{-j\frac{2k\pi}{N}}]^m$. That is, $b_0=\frac{1}{4}$, $b_k=\delta_k(m)$ for $k=1,2,3$, where $\delta_k(m)$ is a function of $m$ such that 
$$\lim_{m\rightarrow \infty}\delta_k(m)=0$$

The reason is that since $b_k\leq |b_k|=\frac{1}{4}[\frac{1}{2}+\frac{1}{2}\cos{\frac{2k\pi}{N}}]^{m/2}$ and $k\neq 0$ then $|b_k|\rightarrow 0$ as $m\rightarrow \infty$.

Now we have

\begin{align*}
\underline{P_{Y_{m}}}_4[n]=\sum_{k=0}^3 b_k e^{j\frac{2k\pi}{N}n}
\end{align*}

Letting $m\rightarrow \infty$, $\underline{P_{Y_{m}}}_4[n]$ converges to $\frac{1}{4}$ and thus $P_{Y_{m}}[n]$ also converges to $\frac{1}{4}$, as desired.

\end{proof}


\section{Proof of Lemma \ref{claim: H_s}}
In order to study the properties of $H_{s}(V,W,r)$ we first need to to assume that $S(V,W,r)$ is non-empty. For fixed $V,W,r$, we can formulate $H_s$ as maximizing :


$$
H(a):=- \sum_{(i,j,k)\in \mathcal{X}\times \mathcal{Y}\times \tilde{\mathcal{X}}} a(i,j,k)\log_2a(i,j,k)
$$
subject to the following conditions:

\begin{enumerate}
\item $\sum_k a(i,j,k)=V(i,j)$, $\forall (i,j)\in \mathcal{X}\times \mathcal{Y}$
\item $\sum_i a(i,j,k)=W(k,j)$, $\forall (j,k) \in \mathcal{Y}\times \tilde{\mathcal{X}}$
\item $\sum_{(i,j,k): k-i=l} a(i,j,k)=r(l)$, $\forall l \in \mathcal{X}$
\end{enumerate}
 Using Lagrange multiplier method we have:
\begin{align*}
L(a):= H(a)&-\sum_{i,j} \gamma_{i,j} (\sum_k a(i,j,k)-V(i,j))\\
&-\sum_{j,k} \lambda_{j,k} (\sum_i a(i,j,k)-W(k,j))\\
&-\alpha_l (\sum_{(i,j,k): k-i=l} a(i,j,k)-r(l))
\end{align*}

Differentiating $L(a)$ gives:

\begin{align*}
\frac{\partial L(a)}{\partial a(i,j,k)}= -\log_2a(i,j,k)-1- \gamma_{i,j} -\lambda_{j,k}-\alpha_{k-i}
\end{align*}

Setting $\frac{\partial L(a)}{\partial a(i,j,k)}=0$ and considering the constraints we get:

\begin{align}\nonumber
a(i,j,k)&=2^{-1- \gamma_{i,j} -\lambda_{j,k}-\alpha_{k-i}}\\\nonumber
V(i,j)&= \sum_k 2^{-1- \gamma_{i,j} -\lambda_{j,k}-\alpha_{k-i}}\\\nonumber
W(k,j)&= \sum_i 2^{-1- \gamma_{i,j} -\lambda_{j,k}-\alpha_{k-i}}\\\label{equ: appx_h_s_lambda...}
r(l)&=\sum_{(i,j,k): k-i=l} 2^{-1- \gamma_{i,j} -\lambda_{j,k}-\alpha_l}
\end{align}

Now let $a^*(i,j,k)$ be a solution for this optimization then at a particular point $(V,W,r)$, $H_{s}(V,W,r)$ is

\begin{align}\nonumber
H_{s}(V,W,r)&= \sum_{i,j,k}[1+ \gamma_{i,j} +\lambda_{j,k}+\alpha_{k-i}]a^*(i,j,k)\\\nonumber
&=1+\sum_{i,j} \gamma_{i,j}\sum_k a^*(i,j,k)+\sum_{j,k} \lambda_{j,k} \sum_i a^*(i,j,k)\\\nonumber
&+\sum_l \alpha_l \sum_{(i,j,k): k-i=l} a^*(i,j,k)\\\label{equ: H_s optimized}
&=1+\sum_{i,j} \gamma_{i,j}V(i,j)+\sum_{j,k} \lambda_{j,k}W(k,j)+\sum_l \alpha_l r(l)
\end{align}

Now treat $\gamma_{i,j}, \lambda_{j,k}$ and $\alpha_l$ as variables. By (\ref{equ: appx_h_s_lambda...}), $\gamma_{i,j}, \lambda_{j,k}$ and $\alpha_l$ can be thought as functions of  $V(i,j),W(k,j)$ and $r(l)$. Since all the functions involved in (\ref{equ: appx_h_s_lambda...}) are differentiable up to any order, then  $\gamma_{i,j}, \lambda_{j,k}$ and $\alpha_l$ are at least second order differentiable. Hence  by (\ref{equ: H_s optimized}), $H_{s}(V,W,r)$ is atleast second order differentiable at a neighborhood of the point $(V,W,r)$.













\section{Proof of Lemma \ref{lem: p_err case2_transversal} } \label{sec: proof of case 2}

In this case:

\begin{align}
P_{err}=\sum_{\substack{v^l, \tilde{v}^l \in \{0, 1\}^l\\ \tilde{v}^l \neq v^l}}  \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)}\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n) \\ \tilde{x}^n\neq x^n}} \frac{1}{2^l} W^n(y^n|x^n) \frac{1}{4^n}\PP\{ (\tilde{v}^l-v^l)\mathbf{\Delta G}=(\tilde{x}^n-x^n)\}
\end{align} 

Suppose $t_l(\tilde{v}^l-v^l)=t$ and $t_n(\tilde{x}^n-x^n)=r$, then by Lemma \ref{lem: p_u case 3}

\begin{equation*}
\PP\{ (\tilde{v}^l-v^l)\mathbf{\Delta G}=(\tilde{x}^n-x^n)\}=2^{-nD(r\| Q_{l,t})-nH(r)}
\end{equation*}

Hence we have:
\begin{align}
P_{err}=\sum_{t\in \mathcal{T}_l(\{ 0,1\})}\sum_{r \in \mathcal{T}_n(\mathcal{X}) } \sum_{\substack{v^l, \tilde{v}^l \in \{0, 1\}^l\\ t_l(\tilde{v}^l- v^l)=t}}  \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)}\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)\\ t_n(\tilde{x}^n- x^n)=r}} \frac{1}{2^l} W^n(y^n|x^n) \frac{1}{4^n}2^{-nD(r\| Q_{l,t})-nH(r)}
\end{align}

Note that since the inner terms of the above equation depend on $\tilde{v}^l, v^l$ only through type $t$, for large enough $l$, we can replace the summation over them by $2^l 2^{lH(t)}$. Now apply the results of  Lemma \ref{lem: p_err3, F_n} for this case to get:

\begin{align} \label{equ: p_e_extra_for v}
P_{err}=\sum_{t\in \mathcal{T}_l(\{ 0,1\})}\sum_{r \in \mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }} \sum_{(x^n,y^n,\tilde{x}^n)\in F_n(V_{XY},W_{\tilde{X}Y},r)}  \frac{1}{4^n} W^n(y^n|x^n)2^{-nD(r\| Q_{l,t})-nH(r)+lH(t)}
\end{align} 

Use Claim \ref{claim: Wn} to conclude that the summation over $(x^n,y^n,\tilde{x}^n)$ can be replaced by $|F_n(V_{XY},W_{\tilde{X}Y},r')|$. By Lemma  \ref{lem: F_n} and a similar argument for defining  $H_s(\cdot)$, bound $|F_n(.)|$ as :

\begin{equation*}
| F_n(V_{XY},W_{\tilde{X}Y},r) | \leq 2^{nH_{s}(V_{XY},W_{\tilde{X}Y},r)+O(\log n)}
\end{equation*} 
 As a result we have :
 
 \begin{align}
P_{err}\leq \sum_{t\in \mathcal{T}_l(\{ 0,1\})}\sum_{r \in \mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }}  2^{-nD(V_{XY}\| P_{XY})} 2^{-nH(V_{XY})} 2^{-nD(r\| Q_{l,t})-nH(r)+lH(t)+nH_{s}(V_{XY},W_{\tilde{X}Y},r)+O(\log n)}
\end{align} 

Now by Claim \ref{claim: H_s} and a  continuity argument, for small enough $\epsilon$ we get

\begin{align}
P_{err}\leq \sum_{t\in \mathcal{T}_l(\{ 0,1\})}\sum_{r \in \mathcal{T}_n(\mathcal{X}) } \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }}   2^{-nH(P_{XY})} 2^{-nD(r\| Q_{l,t})-nH(r)+lH(t)+nH_{s}(P_{XY},P_{XY},r)+O(\log n)}
\end{align} 

 Note that the inner terms in the above summations do not depend on $V_{XY}, W_{\tilde{X}Y}$. Thus summation over them can be replaced by $|\mathcal{T}_\epsilon^n(X,Y)|$ and $|\mathcal{T}_\epsilon^n(\tilde{X},Y)|$. Since these two terms grows polynomially as $n$ grows, they can be ignored to get
 
 \begin{equation*}
P_{err}\leq 2^{nE(n,l)} 
 \end{equation*}
 Where 

\begin{align*}
E(n,l)=\max_{\substack{t\in \mathcal{T}_l(\{ 0,1\}) \\ r\in\mathcal{T}_n(\mathcal{X})}} -D(r\| Q_{l,t})-H(r)+\frac{l}{n} H(t)+H_{s}(P_{XY},P_{XY},r)-H(P_{XY})+O( \frac{\log n}{n})
\end{align*}


As a result of using the consequence coding scheme, discussed in Section \ref{sec: adding ext comp}, $l t(1)\rightarrow \infty$ as $l\rightarrow \infty$. Therefore, for large enough $l$, we can use Lemma \ref{lem: Q_nr convergence} to get the convergence of $Q_{l,t}$ to a uniform random variable in $\ZZ_4$ . Thus, in this case 
 
\begin{align*}
E(n,l)=\max_{ r\in\mathcal{T}_n(\mathcal{X})} -2+\frac{l}{n} +H_{s}(P_{XY},P_{XY},r)-H(P_{XY})+O( \frac{\log n}{n})
\end{align*} 
 
Similar to the proof of Lemma \ref{lem: p_err3, Case 1} one can show that $H_{s}(P_{XY},P_{XY},r)-H(P_{XY}) \leq H(X|Y)$. Consequently:

\begin{align*}
E(n,l)&\leq -2+\frac{l}{n} +H(X|Y)+O( \frac{\log n}{n})\\
&=\frac{l}{n}-I_4+O( \frac{\log n}{n})
\end{align*} 

Thus for any fixed $\delta >0$, if $\frac{l}{n}\leq I_4-\delta$ then, $P_{err} \rightarrow 0$ as $n\rightarrow \infty$. 

Note also that there is  another trivial bound:  $\frac{l}{n} \leq 1$. Hence we get 
\begin{equation*}
\frac{l}{n}\leq \min \{I_4-\delta,1\}
\end{equation*}
and the proof is completed.





\section{Proof of Lemma \ref{lem: p_err case3_transversal}} \label{sec: proof of case 3 transversal}

In this case, similar to the proof for  Case 1, we have:
\begin{align}\label{equ: p_err case 3, transversal}\nonumber
P_{err}&=\sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k\\ \tilde{u}^k \neq u^k}} \sum_{\substack{v^l, \tilde{v}^l \in \{0, 1\}^l\\ \tilde{v}^l \neq v^l}}  \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} \\ 
&\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n) \\ \tilde{x}^n\neq x^n}} \frac{1}{4^k} \frac{1}{2^l} W^n(y^n|x^n) \frac{1}{4^n}\PP\{ (\tilde{u}^k-u^k)\mathbf{G}+(\tilde{v}^l-v^l)\mathbf{\Delta G}=(\tilde{x}^n-x^n)\}
\end{align} 

The following Claim is an extension of Lemma  \ref{lem: p_u case 3}:

\begin{claim}
Let $Z^n=u^k\mathbf{G}+v^l\mathbf{\Delta G}$. Where elements of $u^k, v^l$ belong to $\ZZ_4, \{0,1 \}$, respectively. Suppose $u^k \neq 0^k, v^l\neq 0^l$,  $t_k(u^k)=q, t_l(v^l)=t$ and $t_n(x^n)=r$. Therefore, $Z_i$ are i.i.d and

\begin{itemize}
\item If $q(2)>0$, $\PP\{ Z^n=x^n\}=\frac{1}{4^n}$
\item Otherwise, if $q(2)=0$ 
\begin{equation}
\PP\{ Z^n=x^n\}=2^{-nD(r\| Q_{k+l,q'})-nH(r)}
\end{equation}
\end{itemize}

where $Q_{k+l,q'}$ is the distribution of $Z_i$. Moreover $Q_{k+l,q'}$ depends on $u^k, v^l$ only through  type $q'=(kq+lt)/(k+l)$.
\end{claim}

\begin{proof}
Define $u'^{k+l}=\begin{pmatrix} u^k\\v^l \end{pmatrix}$. Define $t(2)=t(3)=0$. Clearly $t_{k+l}(u'^{k+l})=q'=(kq+lt)/(k+l)$. Also let $\mathbf{G}'=\begin{pmatrix} \mathbf{G}\\ \mathbf{\Delta G}\end{pmatrix}$. Therefore,  $Z^n=u'^{k+l}\mathbf{G}'$ and by Lemma \ref{lem: p_u case 3} the claim is proved.
\end{proof}

In (\ref{equ: p_err case 3, transversal}), let $t_k(\tilde{u}^k -u^k)=q, t_l(\tilde{v}^l- v^l)=t, t_n(\tilde{x}^n- x^n)=r$ and define $P:=\PP\{ (\tilde{u}^k-u^k)\mathbf{G}+(\tilde{v}^l-v^l)\mathbf{\Delta G}=(\tilde{x}^n-x^n)\}$.  As a result of the above Claim, there are two sub-cases for $P$ based on $q$:
 
\begin{itemize}
\item[Case 3.1:] $q(2)>0$,  $P=\frac{1}{4^n}$
\item[Case 3.2:] $q(2)=0$, $P=2^{-nD(r\| Q_{k+l,q'})-nH(r)}$ 
\end{itemize} 
Where $q'$ defined as in the above Claim. 
Consequently, applying the above results, $P_{err}$ can be divided into two terms as


\begin{align}\label{equ: p_err, case 3 trans, part 1}
P_{err}=&\sum_{\substack{q\in \mathcal{T}_k(\ZZ_4)\\q(2)>0}} \sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k\\ t_k(\tilde{u}^k -u^k)=q}} \sum_{\substack{v^l, \tilde{v}^l \in \{0, 1\}^l\\ \tilde{v}^l \neq v^l}}   \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)}\\\nonumber
&\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n) \\ \tilde{x}^n\neq x^n}}  \frac{1}{4^k} \frac{1}{2^l} W^n(y^n|x^n) \frac{1}{4^n}\frac{1}{4^n}\\
%
%
\label{equ: p_err, case 3 trans, part 2}
&+\sum_{\substack{q\in \mathcal{T}_k(\ZZ_4)-\mathbf{0}\\q(2)=0}}\sum_{t\in \mathcal{T}_l(\{0,1 \})-\mathbf{0}}\sum_{r \in \mathcal{T}_n(\mathcal{X}) } \sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k\\ t_k(\tilde{u}^k -u^k)=q}} \sum_{\substack{v^l, \tilde{v}^l \in \{0, 1\}^l\\ t_l(\tilde{v}^l- v^l)=t}}  \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)}\\\nonumber
&\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n)\\ t_n(\tilde{x}^n- x^n)=r}}  \frac{1}{4^k} \frac{1}{2^l} W^n(y^n|x^n) \frac{1}{4^n}2^{-nD(r\| Q_{k+l,q'})-nH(r)}
\end{align}

\textbf{Case 3.1:}
Let $P_{err, 3.1}$ denote the first term in the above (i.e., (\ref{equ: p_err, case 3 trans, part 1})). Similar to the proof of Lemma \ref{lem: part 1&2}, we upper bound $P_{err, 3.1}$ as

\begin{align*}
P_{err, 3.1}\leq& \sum_{\substack{u^k, \tilde{u}^k \in \ZZ_4^k\\ \tilde{u}^k \neq u^k}} \sum_{\substack{v^l, \tilde{v}^l \in \{0, 1\}^l\\ \tilde{v}^l \neq v^l}}   \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)}\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n) \\ \tilde{x}^n\neq x^n}}  \frac{1}{4^k} \frac{1}{2^l} W^n(y^n|x^n) \frac{1}{4^n}\frac{1}{4^n}
\end{align*}



Since the inner term in the summations do not depend in $u^k,\tilde{u}^k, v^l, \tilde{v}^l$, we have:

\begin{align*}
P_{err, 3.1}&\leq 4^k 4^k 2^l2^l \frac{1}{4^n}  \frac{1}{4^k} \frac{1}{2^l}  \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)}\sum_{\substack{\tilde{x}^n\in A_\epsilon^n(X|y^n) \\ \tilde{x}^n\neq x^n}}  W^n(y^n|x^n) \frac{1}{4^n} \\
&\overset{(a)}{\leq} 4^k 2^l\frac{1}{4^n}   \sum_{x^n\in A_\epsilon^n(X)} \sum_{y^n\in A_\epsilon^n(Y|x^n)} W^n(y^n|x^n) \frac{1}{4^n} 2^{nH(X|Y)}\\
&\overset{(b)}{\leq} 4^k 2^l\frac{1}{4^n} 2^{nH(X|Y)}\\
&= 2^{n[\frac{2k}{n}+\frac{l}{n}-2+H(X|Y)]}
\end{align*}

Note $(a)$ is true as the inner terms of the summations do not depend on $\tilde{x}^n$ and thus we can replace the summation over it by $|A_\epsilon^n(X|y^n)|$. Since $y^n$ is a typical sequence, for large enough $n$,$|A_\epsilon^n(X|y^n)|\approx 2^{nH(X|Y)}$. $(b)$ is true by Claim \ref{claim: Wn}.   
Thus $P_{err, 3.1}\rightarrow 0$, as $n\rightarrow \infty$ if :

\begin{equation} \label{equ: case 3.1, bound on l,k}
\frac{2k}{n}+\frac{l}{n}\leq 2-H(X|Y)-\delta = I_4-\delta
\end{equation}
where $\delta >0$ is any fixed number.



\textbf{Case 3.2:} 

In this case, by $P_{err, 3.2}$, denote the second term of $P_{err}$ (i.e., (\ref{equ: p_err, case 3 trans, part 2})). All steps for showing the convergence of $P_{err, 3.2}$ to zero are similar to that of the proof for Case 2 in Section \ref{sec: proof of case 2}. Since the inner terms of $P_{err, 3.2}$ depend on $u^k,\tilde{u}^k, v^l, \tilde{v}^l$ only through type $q'$, for large enough $n$, we can replace the summation over them by $2^l4^k 2^{lH(t)}2^{kH(q)}$. Now apply the result of Lemma \ref{lem: p_err3, F_n}, \ref{lem: F_n} and Claim \ref{claim: Wn} to get:


 \begin{align}
P_{err, 3.2} \leq \sum_{\substack{q\in \mathcal{T}_k(\ZZ_4)\\q(2)=0}}\sum_{t\in \mathcal{T}_l(\{0,1 \})}\sum_{r \in \mathcal{T}_n(\mathcal{X}) }  \sum_{\substack{V_{XY}\in \mathcal{T}_\epsilon^n(X,Y)\\ W_{\tilde{X}Y}\in  \mathcal{T}_\epsilon^n(\tilde{X},Y) }}&  2^{-nD(V_{XY}\| P_{XY})} 2^{-nH(V_{XY})} 2^{-nD(r\| Q_{k+l,q'})-nH(r)}\\
&2^{lH(t)+kH(q)+nH_{s}(V_{XY},W_{\tilde{X}Y},r)+O(\log n)}
\end{align} 

By a continuity argument for small enough $\epsilon$ and the fact that the inner terms in the above summations do not depend on $V_{XY}, W_{\tilde{X}Y} $. Replace the summations over $V_{XY}, W_{\tilde{X}Y} $ by $|\mathcal{T}_\epsilon^n(\tilde{X},Y)|, |\mathcal{T}_\epsilon^n(X,Y)|$. Since these two terms grows polynomially with $n$, conclude that:

\begin{equation*}
P_{err, 3.2} \leq 2^{nE(k,l,n)}
\end{equation*}

where

\begin{align*}
E(k,l,n)= \max_{\substack{q\in \mathcal{T}_k(\ZZ_4)\\q(2)=0}} \max_{\substack{t\in \mathcal{T}_l(\{0,1 \})\\ r \in \mathcal{T}_n(\mathcal{X})}} -D(r\| Q_{k+l,q'})-H(r)+\frac{l}{n} H(t)+\frac{k}{n}H(q)+H_{s}(P_{XY},P_{XY},r)-H(P_{XY})+O( \frac{\log n}{n})
\end{align*}

As a result of the consequence coding scheme proposed in Section \ref{sec: adding ext comp}, we can assume that $k q(1)$ and $kq(3)\rightarrow \infty$ or $l t(1)\rightarrow \infty$  as $l, k\rightarrow \infty$, then $(k+l)q'(1) or (k+l)q(3) \rightarrow \infty$. Now use Lemma\ref{lem: Q_nr convergence} to get the convergence of $Q_{k+l,q'}$ to a uniform random variable in $\ZZ_4$ for large enough $k+l$. Thus, in this case:


 \begin{align*}
E(k,l,n)&= \max_{\substack{q\in \mathcal{T}_k(\ZZ_4)\\q(2)=0}} \max_{\substack{t\in \mathcal{T}_l(\{0,1 \})\\ r \in \mathcal{T}_n(\mathcal{X})}} -2+\frac{l}{n} H(t)+\frac{k}{n}H(q)+H_{s}(P_{XY},P_{XY},r)+O( \frac{\log n}{n})\\
&= \max_{\substack{r \in \mathcal{T}_n(\mathcal{X})}} -2+\frac{l}{n} +\frac{k}{n}\log_2(3)+H_{s}(P_{XY},P_{XY},r)-H(P_{XY})+O( \frac{\log n}{n})
\end{align*}

Similar to the proof of Lemma \ref{lem: p_err3, Case 1}, one can show that $H_{s}(P_{XY},P_{XY},r)-H(P_{XY}) \leq H(X|Y)$. Consequently:

 \begin{align*}
E(k,l,n)\leq -2+\frac{l}{n} +\frac{k}{n}\log_2(3)+H(X|Y)+O( \frac{\log n}{n})
\end{align*}

Thus, as $n\rightarrow \infty$,$P_{err, 3.2}\rightarrow 0$ if :
\begin{equation} \label{equ: case 3.2, bound on l,k}
\frac{l}{n} +\frac{k}{n}\log_2(3) \leq 2-H(X|Y)\delta=I_4-\delta 
\end{equation} 
where $\delta >0$ is any fixed number.

Finally having (\ref{equ: case 3.1, bound on l,k}), the above bound is redundant and the proof is complete.
